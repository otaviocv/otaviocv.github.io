[{"url":"https://otaviocv.github.io/blog/","title":"the index","body":""},{"url":"https://otaviocv.github.io/blog/real-time-machine-learning-tips-and-tricks/","title":"Real-time machine learning: tips and tricks","body":"\n\t\n\n\nMore than an year ago I developed a presentation summarizing all the practices,\nkindly named as tips and tricks, for maintaining and operating real-time\nmachine learning models at the company I work for. This was first an internal\npresentation that turned into a meetup presentation that turned into an editted\narticle. After more than one year of compiling this knowledge I decided to\nrevisit them and record an updated version of this text in my own website.\nYou can check the resources for the meetup talk, in portuguese, and the editted\narticle, in portuguese and english.\n\n\nPractices to scale Machine Learning operations\nThe real-time subset of this article\nReal-time machine learning can represent various types of applications and\ndue this wide range of possibilities I would like to precisely describe what\nI've been doing such that you can make the appropriate approximations\nto other contexts.\nI work with fraud prevention in a financial institution.\nThis means that I work developing automated decision systems\n(not necessarily just machine learning models) that identify\nevents that shouldn't happen, either by a legal violation or by a third party\nmalicious action. The key element here is that to provide good customer\nexperience and avoid tragedies is that we must identify the fraud in a given\nevent at moment it is happening. At the exact \"approval\" time, otherwise money\nmoves too fast for you to catch up later.\nThe most obvious and direct example is a credit card transaction. Imagine you\nbuying something at a sketchy online store, you know it is sketchy but\nthe price is too good to not take. You buy the thing and it never arrives at\nyour door. When you check the store again, it disappeared. A couple days later\nmisterious purchases notifications reach you and you know exactly how your card\nnumbers were leaked.\nApplying machine learning to this problem means that at the moment a credit\ncard is getting our systems, real-time, you must decide to approve it or not.\nThe challenges of maintaining such systems start with respecting the SLA,\nusually milliseconds or seconds, and applying this analysis to millions\nof customer and pontentially billions of transactions.\nMy context is hyper focused in building models and systems that\nscale very well to millions of customers, billions of transactions, and\ncan answer synchronously in milliseconds.\nA comparison with Batch models\nTo highlight the real-time elements of my context I like to compare with\nbatch models which, in my experience, are the preferred first approach\nto any team trying to adopt automated decisions.\nBatch models usually run in a fixed schedule, every once in a day,\na week, or a couple of hours. They take all the new instances that\nwere generated in the last time window and apply their predictions\nto it. They are deployed in ETL/ELT systems and consume very big tables\nusually. Real-time models consume multiple one row tables in the\nother side.\nThese input tables for batch models may be composed of multiple\nother tables and upstream sources that combined by a batch processing\nsystem like Spark, Pandas, Databricks, Big Query. Meanwhile, real-time\nmodels seek information in various forms, feature store like systems\nwith pre-computed data, batch tables that were loaded in some form\nof fast database, the true source of the data in the form of a\nmicroservice and its APIs or other intermediate aggregation\nservices that hold temporary, short-lived data, just for the purpose\nof building the features for a real-time model.\nYou can see that batch models can take up to the interval time duration\nto finish its predicitions in this comparison. They have a lot more room\nto fail, you can retry multiple times, and failures are not directly or\ninstantly forwarded to end customers.\nThe real challenges of maintaining real-time models don't live in the\nmodel itself, there are challenges at optimizing model python code, but\nrather in retrieving all the features required for its usage.\nOne possibility for building micro services that hold models\nThe number of different possible way you could arange architectural\ncomponents in order to having a functional decision making system is\nprobably greater than the number of Rubik's cube states combined with\nthe number of chess boards, specially when you consider how hard\npeople's opinions are on various subsets of \"systems architecture\".\nIn this section I will describe the particular choices made at the company\nI work and try to justify them. Some of these justifications will be\ncompletely local and historical to the company's context in time and space,\nso don't take them so hard.\nThe other language\nI expressed very briefly my opinion that we don't get to choose any of the\nprogramming languages that we use professionaly or that just a very small\ngroup of people have the privilege of making this decision.\nIn the company I work for it is no different. The weapon of choice was\nclojure! So, the first thing to keep in mind when design automated decision\nsystems is that the entire company tooling, automated checks, native\nintegrations were created for clojure micro services.\nThis effect increases a lot the friction to break the clojure standard and\nthe practicity to integrate with other platforms and solution is also\ngreatly improved.\nAs you can obviously imagine we are not deploying our machine learning models\nin clojure. We are developing, training and deploying them in python! The\nmain reason is that there is just so much of machine learning work already\ndone in public available packages and it is the standard of the industry.\nThe outcome of these two elements is a two language design. There is the\nmajority of services written in clojure and very few services holding\nmachine learning models written in python.\nThe design\nWe choose to minimize the python \"surface area\" in the systems. Everything\nthat can be done in clojure should be done in clojure. To be clear, retrieving\ndata from other services, performing aggregations on historical data, and\napplying decisions to model predictions are all task that we decided to execute\nin clojure.\nWhat is left to \"python\" is just holding a thing layer of translation components\nand the model it self. We try to keep its interface as close as possible to a\n\"feature vector\" or a flat json schema from feature names to feature values.\nStill, there is a wrapper written in clojure that holds \"common tasks\" that\nall models should have like authentication, propagation of predictions to kafka\nand the ETL. This bundle is joined together in a Kubernetes pod applying the\n\"side car\" pattern.\nWith this design we achieve:\n\nNo python directly exposed to other services, just clojure.\nMinimal python deployment. Just the model, nothing else\nAnything that we need to apply to all deployed models\ncan be included in the \"wrapper\" and instantly adopted.\n\n\n\t\n\n\nThere are downsides to this but I am not prepared yet to write about them.\nSoon, I will articulate the limitations of this design and what would be\nthe next generation of this architecture for the next 10 years. Some\nof these limitations are already explained in my\n\"The future of Machine Learning\"\narticle.\nA closer look at the Model\nLet's discuss a bit more in focus what is happening in the model pipeline.\nWith this minimalistic approach in mind, let's expand each section:\n1. In Schema\nThe default format for inter service communication is json.\nThe default choice for the input schema is flat structure with feature\nnames as keys and feature values as values. There are other choices\nThere are other choices to this like when you have to aggregate something\nto turn into a feautres, like when you want to count and sum the amount\nof a transaction list. You have two basic choices: to aggregate in the\nrequesting service or to provide a the raw list and aggregate inside the\nin adapter or the model pipeline.\nCertainly, if you can do more work in the model pipeline the gap for\ndiscrepancies between batch results will be smaller but that can\npontentially break one assumption that we like a lot: \"n rows in, n rows out\".\nThis assumption is the assumption that if you send N instances of prediciton\nyou must get back N instances with their predictions. In our practices\nit feels odd to send 10 rows in and get only one row back, specially when this\nwork must be executed in the adapter because, for optimization purposes, you\ndecided to aggregate this in a dataset or a datapipeline outside the model\npipeline.\nBreaking this assumption also complicates the usage of the ETL\nlogging machanisms, that also assume this. It requires you to start working\nwith list values inside parquet tables and then reapplying the in-adapter in these\nvalues to reproduce what the model received. Awful thing.\nIf you end up creating a big in-adapter you increase the gap for discrepancies\nand start doing a lot of work at a place we don't have the same tools to tackle\nproblems. Python was notably bad at asynchronous features\n(*until 3.14 at least, let's see how things evolve) and tackling multiple\nheavy liffiting tasks in the in adapter seemed like an objective worse option\nthan doing all the pre-aggragation tasks in the requesting service.\nSo, in general we like:\n\nFlat schemas. {\"feature-name\": value}\nOne prediction instance in, one prediction instance out.\n\n2. In Adapter\nAgain, the default format for inter service communication is json.\nThe first step of getting model predictions is to translate from json to\nsomething the model can understand. Our default choice is dataframes,\nPandas dataframes.\nThe first key element of this in-adapter approach is to keep the model interface\nuniform. The model is a sequence of steps that always gets a table as input\nand returns a table as output preserving the same input schema and values but\nadding new coluns to it: the raw prediction, some kind of calibrated prediction and\nfitted empirical cumulative distribution values (ECDF) are options.\nThat's why keeping the flat schema is so convenient, it simplifies a lot the\nin-adapter.\nWe turn this flat schema into a single row dataframe and forward it to the\nmodel. Even the assembly of all these components, in-schema, in-adapter,\nthe model, out-adapter, and out-schema are not hand made. There is an\nassembler template that, if you are not going to do anything different\nfrom this pattern, automatically assemble the pipeline from a config file\nwith default implementations.\n\nA Note on Pandas\nPandas dataframes are notably slow. They set a historical mark when first\nreleased but now a days they feel outdated. With the recent advancements\nof the arrow backend you can mitigate performance issues but Polars was\na true hit. Beyond providing a much performant implementation its API\nshould be the standard API for dataframes for me.\n\n3. Model\nThe model is the serialized artifact generated by the training process.\nFor this discussion you can assume we have a standard way of training\nand serializing the model, we use pickle, that can later be referenced\nat deploy time and dynamically loaded into the pipeline temapled mentioned\nin the in-adapter section.\nWhat is important to highlight is that the model pipeline it self is\nanother sequence of steps with the \"table in, table out\" interface.\nshort detour: fklearn\nThis \"table in, table out\" is the interface introduced by fklearn,\na functional inspired interface for machine learning pipelines. It\nis open sourced and the details of its creations can be checked in\nthis two part post in the company's blog:\n\nfklearn part 1\nfklearn part 2\n\nYou can also check:\n\nfklearn github repo\nfklearn pypi page\n\nfklearn introduces a bunch of wrappers to usual sklearn, xgboost,\nlightbm, catboost and tensorflow packages that always follow the same\nAPI:\n\nThis uniform API let's us create uniform pipelines without concerning of their\nown methods and representation, basically every machine learning framework has\nits own data type. We just translate in, train, and translate out and we can\nwork confident that any fklearn building block will respect this API.\nThis is an extremelly powerful abstraction that simplifies a lot the\nconstruction of complex model pipelines, specially when you need to\ndo more after work after training the \"main\" model. Still may not\nwork for LLMs but extremelly convenient for building medium and large\nscale models.\n(I make a comment about model size in my\n\"The future of Machine Learning\")\nThe usual practice, when testing and adopting new frameworks, is to simply\nwrap them in the fklearn interface and introduce in a existing\nmodel pipeline or swap a previous implementation.\nYou can see that in this description\nabsolutely everthing remains constant, which improves a lot productivity\nin the experimentation, prototyping and deployment phases.\nBack to model pipelines\nWith the fklearn mode of building models loaded let's break down the model\npipeline as a simple sequence of steps:\nLet's call the \"model\" as \"model pipeline\" and call the main\npredictor/learner in the pipeline as the \"model\":\n\n\t\n\n\nThis is the heart of the entire prediction process and the step that should\ntake the most of the prediction time. It was a true suprise when some people\nstarted profilling various sections of the pipeline to realize the requests\nwere 80%+ of their total time in the schemas and adapters!\n4. Out Adapter\nAfter we get the single row output dataframe, we translate it back to a dictionary\nform in other to serialize it back again to json. In the output adapter we usally\nthrow away most of the features and keep just what is important for taking downstream\ndecisions. When it is absolutely necessary to keep the all the output, we build two\ntop level keys: what is relevant to the requesting service and what is only\nrelevant for the etl propagation.\nCoercing types back to something that is json serializable is one of the most boring\ntasks I have done in my machine learning engineer life.\n5. Out Schema\nFinally we validate the structure of the output of the out adapter as sanity\ncheck. If it fails we return a 500 and that is life. Usually, we can prevent\nmost of the serialization issues in the adapter but who knows what can happen.\nThe other services have a very interesting contract\n(check this article on how they done it:\nWhy We Killed Our End-to-End Test Suite)\nbased testing tool but, as mentioned, it only works for clojure services.\nWe hope to use such structure in the future but not possible for now.\nSummary on the model deployment structure\nThis is a very uniform and standard way of approaching the deployment of\nmachine learning models inside services. I will disclose and discuss a bit\nmore the technology choices of all these stages but what is important to\nknow is that if you are building a \"standard\" model, i.e. a model that\ncan fit the described structure, you get the entire deployment almost for\nfree with just minor configuration stages. This yields a great uniformity\nover all real-time models and great productivity for both Data Scientists\nand Machine Learning Engineers.\nWithout more blabla,\nLet's begin the tips and tricks!\nThe techniques\nCost optimization, or just doing less\nRemember of unnecessary redundancy\nOne of the strategies to scale adopted by the company was to apply sharding,\nnot just for databases\n(Managing Cloud Limits) but\nfor everything, including services, kafka and models.\nThe issue with sharding is that although it works very well for scaling databases\nit may introduce unnecessary redundancy for shareable workloads specially\nstatless services like model services. Given that shards are relatively independent\nand that, in order to keep high availability, we required a minimum of 2 independent\nwork units (pods in Kubernetes), if we have 20 shards we already starts with a minimum of\n40 working units.\nFor many usecases even using fractions of resources was already a lot making the\nbottom of costs very high just to start. Besides that, tasks that operates with\na shard bias would be under utilized in low load shards and under pressure in high\nload shards (there is the possibility to make different resource allocations per\nshard but we took an even better approach). We observed many times models going\ndown because of hicups in resource allocation, one pode goes down, the other\ngets overloaded and model down.\nThe strategy to overcome that was to get rid of sharded model deployment. Take a global\ndeployment setting where requests from all shards goes to a single global set of model\npods. This reduced the minimum, since now we can keep the minimum at a global level,\nand reduced the under utilization buffer since shard bias didn't matter anymore.\nThis was also good to improve mini batching efficiency since we could keep latency\nrelatively the same value but increasing the through put.\nFilter as much as possible\nJust doing less is the best optimization possible. The trick we employ is,\nwhen possible, to create some kind of rough filter, often called \"pre policy\"\nto control model eligibility and pontentially save precious units of work.\nIf a given customer or transaction can be considered \"not applicable\" from\nother fast acquiring features and in simple logic form we can return a\npreliminary, simpler, result and not waste all the resources required\nto run a model. Otherwise, we proceed to a regular model scoring.\nThe effect is dramatic. Here are some examples (as of October 2024):\n\nThe identity fraud model that runs in various events\ngets reduce from a raw count of events of 2800 events per second to\nonly 20 events per second.\nThe theft model is designed to work in the same events. With its\nfilter we can reduce to only 200 events per second.\n\nThis technique is not always possible to be applied. Sometimes you\nare trying to identify exactly what you designed your model for and\nif a pre-policy was possible to be done you wouldn't be building the\nmodel in the first place.\n\nThe credit card transaction model can't use a pre policy.\nThe mule accounts model can't use a pre policy.\n\nOptimize your dependencies graph\nDependencies graphs can be the source of countless problems and headaches.\nThis is one typical dependency graph from one of the models that run in my\ncompany:\n\n\t\n\n\n\nIn this ascii art we see a representation dependency graph. Each circle\nrepresents a request to an external service to fetch information required\nto build the final set of features. We start from the top, with imediately\navailable information like the id-1, id-2, id-3, id-4, and we proceed\ndown reaching out for features. The lines represent dependencies, so if a circle\ncomes above it must finish before we can execute a circle below.\nIn some cases some intermediate information is required before we can reach\nto the information we want. This is typical to cases where you have a secondary\nidentifier and you need to get the primary identifier to then query the\nmain table for the features. Some times you need two identifiers,\nonly the combination of them fully identifies the entity you are looking for,\nto find the desired features.\nAs you can see, there are 32 requests to external services to fetch\nfeatures. The longest dependency chain has 4 requests in sequence.\nAs we you may imagine, if we execute these requests in sequence, and\neach one of them takes 100 milliseconds to complete, we would wait\na total of 3200 milliseconds to get all features ready. In this\nparticular case the model was required to answer in less than 700\nmilliseconds.\nIn order to achieve that, we paralelize all the requests with\nnative clojure async features, future and delays. This way the total\ntime to retrieve all features gets bounded by the longest chain, 400\nmilliseconds in this case, leaving 300 milliseconds for the additional\nmodel request and the logic that turns model predictions into actions.\nKeep your controllers simple\nBut building this type of controller gets complicated really fast and your logic start to\nget completely filled with deref operators and keeping track of errors in deep chains of\ndependencies becomes a hard task. Also, sometimes you need to transform previously\nfetched information into a new representation in order to call the next service, which\ninterperse you controller with impure side effects pure logic computations, making really\nhard to test the full behavior and map all corner cases.\nRecently, the company open sourced nodely, a tool\nbuilt exactly to tackle these complex graphs and assist separating effectful\ncomputations from pure logic, providing a simpler mental model and better interception\npoints for testing. It topologically sorts the graph and optimally executes it for you.\nMaintaining nodely graphs is still hard, as it is to build and represent the entire\ncomputational graph, but much simpler than hand crafting them from vanilla clojure\n(future and delay), or core.async building blocks. One of the nice features of nodely\nis that it supports many backends and styles of actually exectuing the graph.\nIf you are also building complex feature or dependency graphs consider taking a look at\nit, not just for the library itself but it also presents a comparison with similar work\nand the differences between them.\nUse as much as possible asynchronous techniques\nKeep tight control of timeouts\nUse something clever to do the hard work for you\nMonitor them closely\nGo beyond your borders for optimal performance\nOptimize the model service and the model pipeline\nSimplicity is the key to success, or how to avoig being overwhelmed by your own complexity\nSplit long term and short term features\nRely on know tools, systems and sources\nReduce the feature engineering implementation gap\nJust when valuable, build your own thing to store data\nTests and ergonomy\nGood integration tests make cents\nTest in production always! But mitigating risks\nFinal thoughts and conclusion\nOne size doesn't fit all\nReferences used\nStars\n"},{"url":"https://otaviocv.github.io/blog/on-the-future-of-machine-learning/","title":"The future of Machine Learning","body":"\n\t\n\n\nThis is a text that I've been long waiting to write. After long reflections with my self\nand probing various friends and coworkers I will give it a chance of becoming something.\nI want to talk about the vision of a different future, a future where my life is easier\nat training and deploying machine learning models.\nI though of many titles for this post: How to make my life less miserable?,\nMachine learning for the rest of us?, Interoperable and scalable machine learning.\nPick the one you find the best.\nIn this post I will introduce a few issues that I could observe from my year of experience.\nThese are:\n\n[Small, Medium and Large. But not Huge]\n\nThe diagnosis\n1. Small, Medium and Large. But not Huge\nIn the past few year LLMs got a lot of attention and put Artificial Intelligence in the\nspot light. I don't deny the importance of LLMs and their wonderful applications but I\nwill argue that not a lot of people are building these massive language models at absurd\nmassive scale.\n\nJust recently it was announced by OpenAI a 500 billion investment\nin this Stargate data center. I can't even magine how much computational power such system can\nhold.\n\nWhat I am mostly concerned is what I am going to call Small, Medium and Large scale\nmachine learning.\nLet's think about all of us, ordinary machine learning engineers and data scientists,\ntrying to build all sorts of specialized models. We can think of insurance models,\ncredit models, all sorts of text and image classification models, the list is long but\nthe common factor is that even the most complicated specialized image models are not\neven near the billions of parameters that LLMs are building.\nWe are deploying these models are batch jobs, simple http services, inside streaming\napplications or even in low power hardware like Raspberry PIs and ESP32.\nIs it ok to think about training these models in my notebook? Using some simple\ninteroperability to deploy these models in other infrastructures and follow my life\nwithout getting worried that these models are too expensive? Or that they are going\nto wake me up at night?\nI will claim, based on nothing, that the majority of the machine learning practitioners\nare building small, medium and large models. But not huge. We have scikit-learn, all\nthe boosting libraries, we have a lot of new work in new deep learning frameworks but\nat the end, did our lives got easier in the past ten years?\nThe company I work for kinded of sorted out a way to minimize friction on pickling and\ndeplicking stuff, controlling environments and requirements. But I heard still companies\nusing massive monorepos of models with requirements fully pinned. Batch jobs still have\nmiserable slow performances and are way more wasteful than they need to be\n(are you loading your entire inference dataset in the memory to apply an old model?).\nIt is clear that pinning requirements is not a sustainable solution, but if you are not\ndoing that, how are you managing legacy models? It is good to reproduce some old predictions\nto compare historical stand points but then we need to keep storing legacy code that must\npair with these legacy pickles and/or serialized weights.\nThis doesn't seem practical neither conveninent. So, in order to tackle this \"bellow huge scale\nmodels\", I will propose a new model (not a machine learning model) of deploying these decision\nmachines that take the usual reality of the industry into consideration: the machine learning\nmodel interoperability.\n2. Why do we need interoperability?\nI don't believe all the machine learning practitioners feel that Python is the best programming\nlanguage for everything. The main reason why Python is the \"AI Language\" is because it became\npopular before anything else. Python is going to last for ever but I believe it will remain\nfor a log time still, at least more 20 years.\nBut that doesn't preven us from chosing a better language for everything else, right? I can pick\nmy good and old Java, or Go, or Elixir, or whatever is trending right now. The thing is that as soon\nas you want to integrate automated decisions backed by machine learning models (that definetely\nare going to be developed in Python) you will need to break your language choice. Beyond that\njust a small set of machine learning engineers get to pick their data pipeline language or their\nbackend language, these are decisions left often to founders at very early stage in companies.\nIn older companies and bigger enterprise you just can't throw in a new language.\nSo, most of us don't chose the language the find the best for the problem but whatever\nlanguage chose year before.\nIf we want to make our life simple and easy we must find a way to take the trained models\nand embed them in other programming languages, either for batch processess, streaming\napplications, backend services, games or whatever you are builidng.\nDuckDB: a little digression with a spark\nI recently started testing DuckDB from clojure environments. I was attempting to replicate\ndataframe based feature engineering pipelines in clojure and DuckDB, through the Ibis Project,\nseemed like an approachable alternative.\nWhile reading the documentation and understanding how it could work I saw that DuckDB could\nbe integrated in the JVM environment using a single binary! Using the C foreign function\ninterface.\nI copy the binary in my project, dynamically link it in runtime, and we get DuckDB running!\nSuper fast, simple and easy. This experience was so refreshing that a spark happened in my\nbrain.\nWhy can't I distribute machine learning models as stand alone binaries?\nStand alone binaries as the main distribution format of machine learning models\nIf we can stablish a universal inference API, with a universal in memory table inter process\nlayout, we could make predictions without even copying any data and without relying on any type\nof serialization.\nThere are projects like ONNX and Safe Tensors that try to map all sorts of neural networks\nlayers and dags but at the end the spec must be always at pace with what is possible\nto describe in the deep learning frameworks. If I am an academic, could I write my new\ntrendy and cool neural network layer and make it instantly available by just compiling it?\nFor sure the compiled models would still be limited by what is possible to compile but\nif we introduce a more high level standard API that libraries could built upon we could\nstart a platform of more interchangeable decision systems.\nStand alone binaries are completely detached from their training code, making possible\nto keep legacy stuff without storing environments or keeping control of multiple\nmoving parts.\nDynamically linking could be combined with \"model managers\" that can perform smooth rollouts,\nA/B tests. MLEs can choose if they deploy a model with dedicated resources, or they are so light\nthat they can be embedded in other services to keep complexity low. In device deployment\nbecomes struggle free. IOT devices and limited hardware can pick special compilation flags\nto prune as much as possible of the final executable.\n3. The historical split between deep and classical learning\nI don't know why, but machine learning frameworks have been divided into big categories:\nclassical frameworks (scikit-learn and boosting libraries) and deep learning frameworks\n(tensorflow, pytorch, jax).\nThis is an annoying reality for me because frequently I find my self trying to mix and\nmatch these tools. For me it seems completely reasonable to make simples feature engineering\nto my standard features before jumping to the complex neural network. Like, simple\nregex features work and I don't need fancy text encodings or embeddings. But couldn't we just\nhave a uniform estimator API that can join classical and deep learning frameworks\nin a single cohesive ecosystem?\nThe place I work for, Nubank, has very interesting machine learning library called fklearn.\nThis library adresses this very critical inconvenience of having multiple data standards.\nIt wraps all the common builidng blocks of machine learning, including deep learning,\nwith a uniform Pandas interface. Any estimator has an interface of data in, state out.\n\nBut as already described, it is just a wrapper, it keeps translating from Pandas to the internal\ndata model of each framework:\n\nNumpy for scikit-learn\nPropietary data format for boosting\nWeird tensors for pytorch and tensorflow\n\nScikit-learn comment on data\nI followed scikit-learn project from development versions from 0.19.0 up to its first major release.\nThe drama along either supporting or not Pandas dataframes reduced its ability to become the\nstandard for anything else. Its API had limitations to incorporate early-stopping techniques\nand a lot of potential was left.\nI understand the dilema of including an unknown dependency to the project and\nwidening the data api to a whole new format but that said DataFrames are\nthe entrypoint for analytical jobs today.\nAnyone that is building a model will start by loading data in a dataframe.\nBack to unifying machine learning frameworks\nCould we have all machine learning libraries built on top a single unified\ndata format, with a unified top level API, that let us mix and match\nany kind of estimator and custom implementations from multiple libraries?\nWhy not store tensors as usual DataFrame columns? And use them as regular\nfeature values for models? Can you imagine a dataframe column with a datatype\n\"Video\"? Is that too crazy?\nIf these memory standard can be spread beyond the Python ecosystem, we get our\ninference API from simple reasoning.\n4. Unlocking the potential of modern hardware\nThe Rust revolution arived the big data world leaving nothing behind. But not just Rust,\nZig and old C/C++ for those willing to let go modern features.\nThe world of low level optimization and the practice of squeezing every inch of performance\nfrom chips and leveraging GPU technologies is making Spark and Parquet looking like a\n2014 Honda Civic near an acessible ultrafast EV.\nAll sorts of projects like DataFusion, DuckDB, Polars, Arrow, Vortex, Feldera, IceBerg, TigerBeetle\nare acomplishing things never imagined 10 years ago by reimagining specialized domains\nand leaving behind legacy constraints.\nCould we take the learnings and practices from these projects, levereging their discoveries,\nand apply to the next machine learning library? Seeking a middle ground between academic\ndevelopment and industry strength solutions? What was the ground breaking thing\nin machine learning libraries in the last 10 years? Certainly pytorch,\ntensorflow and scikit-learn itself. But what is going to be the next one?\nThe dream of a new machine learning library\nGiven this diagnose elements I will try to design a new machine learning\nlibrary that can potentially be the breakthrough of machine learning\npractitioners in the next 10 years.\nThe requirements are:\n\nPython is non negotiable. It is the standard for practitioners, almost the front end\nof model builders. Anything for the future must, at least, expose a Python API.\nA uniform memory model that cuts the necessity of multiple translation steps. Once\ndata is ready to be trained only incremental changes are required. The current standard\nfor analytical in memory data representation is the Arrow Project.\nPerformance is non negotiable. Whatever is going to be built must be strongly\nbenchmarked and the most efficient algorithm and implementation.\nA high level API that simplify composition from heterogeneous estimators. No DAGs, just\na sequence of steps. Parallelism can be handled at row chunks level.\nA uniform exposition of hyperparameters space for automatic hyperparameter tunning\nalgorithms.\nA compile API that exports JIT models as stand alone binaries, completely detached from their\ntraining code. This can also include optinal interface systems like streaming apis, http apis,\nbatch inference apis.\nClients in various programming language ecosystems to save as much time as possible from our\nfuture users.\nRust is our weapon of choice for low level implementation and guaranteer of the following requirements.\n\nThe break in\nWe must start somewhere. My proposition will be to build a custom implementation for fast frugal trees,\na model class that I recently got in touch for work and can be an example to sort out the details\nof these requirements.\nThank you!\nCarrot\n"},{"url":"https://otaviocv.github.io/blog/releasing-the-blog/","title":"The challenge to be viewed","body":"\n\t\n\n\nIt has been a while since I started thinking on building a webstie, a blog, a form of digital presence that is\nout of social media or the closed internet. Since the release of GitHub Pages this desire exists and since this day\nI involved myself in the development of multiple prototypes of websites with countless differente frameworks:\njekyll, hugo, react and svelte, using Vercel, Netlify... and now Zola. I can't remember how many times I deleted and\nrecreated my otaviocv.github.io repository.\nMany of these attempts didn't come to life for various reasons, I didn't like the themes,\nI wished multilanguage support, katex, having nothing to say or just the embarassement of exposing yourself.\nAfter much internal thinking, noting that newer frameworks enable a much simpler and pratical website and\nletting go of an aesthetic ideal this website comes to the world.\nIt is built with Zola and the Duckquill theme,\nit has an interesting aesthetics, support to many languages and\ncomments using Mastodon. One of the reasons of this introductory post is to test this integration.\nToday, my necessity to have a space to speak is greater than never and I believe to be able to say things\nthat matter to other people. The modern times required, also, a constant showcase and personal brand, a professional\naesthetics, that emulates the form of producing content for social media. This website intends to be all these things\nbesids also being a big index to all stuff I do.\nWithout artificial intelligence, I hope that here becomes a place of authenticity, learning and discussion.\nSee you there!\nFlowers\n"},{"url":"https://otaviocv.github.io/","title":"otaviocv","body":"\n\t\n\n\n\n\nHi! I am otaviocv, I am from Brasil and I think a lot.\nI am graduated in physics and for the past 8 years I've\nbeen working as a Machine Learning Engineer. My mind wander\nto various places covering topics from very unrelated places like functional\nprogramming, type systems, compilers, cad software, design, puzzles, data engineering,\neditors and, obviously, machine learning.\nThis website was built to share ideas, create dialog spaces and be my place of free expression.\nMy plan is to share a few recurrent thoughs, prototypes, ideas in a more structured\nway and, hopefully, create a comunity on this specific style of presentation.\nThe blog section should be faced as a linear feed of acvities sumarizing\neverything in a single timeline. I made my best to localize everything but certain\ntopics aim specific audiences and may not be worth translating to all languages.\nBe welcomed!\n"},{"url":"https://otaviocv.github.io/interesting/","title":"Interesting things I found","body":"Following the idea of the small web\nand keeping a small list of references that influences me this is a mental log\nthat stores what I share with my family, friends, coworkers and, now, you.\nPodcasts\nEnglish\n\nSignals and Threads\nDisseminate: The Computer Science Research Podcast\n\nYoutube\nChannels\nEnglish\n\nCMU Database Group\n\n"},{"url":"https://otaviocv.github.io/backlog/","title":"the backlog","body":"The place to store the spark before it fades away...\n1. Post more things!\n\n2025-10-26 Meetups presentations!\n\nThey are already there, only in Portuguese:\n\nReal Time tips and tricks: how to scale your machine learning practice.\n\n\n\n\n2025-10-26 The machine learning future\n\n2. 2025-10-26 Variant Sudoku Solver\nI spend a lot of time watching Cracking The Cryptic\nwhere Simon and Mark solve variant sudokus, sudokus\nthat include more rules than the original 1 to 9\ndigits contraints on columns, rows and boxes, therefore I would like to explore\nthe idea of applying the contraint propagation method from the Peter Norvig post\non solving sudoku with additional rules.\n"},{"url":"https://otaviocv.github.io/about/","title":"About","body":"\n\tIf you are looking for a formal Resum√©, please, refer to my\nLinkedIn page.\n\n\n"}]